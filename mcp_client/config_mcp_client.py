from __future__ import annotations

import asyncio
import json
import os
import traceback
from contextlib import AsyncExitStack
from typing import Dict, Any, List, Optional

import yaml
import requests
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


# ========= ƒê·ªåC CONFIG =========

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)
CONFIG_PATH = os.path.join(ROOT_DIR, "config.yaml")


def load_config() -> Dict[str, Any]:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


# ========= OLLAMA HELPER =========

def call_ollama_chat(
    base_url: str,
    model: str,
    user_prompt: str,
    system_prompt: Optional[str] = None,
) -> str:
    url = f"{base_url}/api/chat"

    messages: List[Dict[str, str]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_prompt})

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    try:
        resp = requests.post(url, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        return f"‚ùå L·ªói g·ªçi Ollama: {e}"

    # Chu·∫©n OpenAI style
    if "message" in data and "content" in data["message"]:
        return data["message"]["content"]

    if "choices" in data and data["choices"]:
        return data["choices"][0].get("message", {}).get("content", "")

    return json.dumps(data, ensure_ascii=False)


def call_ollama_for_json(
    base_url: str,
    model: str,
    user_prompt: str,
    system_prompt: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Gi·ªëng call_ollama_chat nh∆∞ng y√™u c·∫ßu LLM tr·∫£ JSON thu·∫ßn.
    C√≥ th√™m l·ªõp parse robust.
    """
    raw = call_ollama_chat(base_url, model, user_prompt, system_prompt)

    # Th·ª≠ t√¨m block JSON trong raw
    try:
        # N·∫øu chu·ªói ƒë√£ l√† JSON
        return json.loads(raw)
    except Exception:
        pass

    # Th·ª≠ c·∫Øt t·ª´ d·∫•u { ƒë·∫ßu ti√™n ƒë·∫øn } cu·ªëi c√πng
    try:
        start = raw.index("{")
        end = raw.rindex("}") + 1
        return json.loads(raw[start:end])
    except Exception:
        # Cu·ªëi c√πng: tr·∫£ fallback
        return {"tool": "none", "arguments": {}, "raw": raw}


# ========= GENERIC MCP CLIENT =========

class ConfigMCPClient:
    def __init__(self, config: Dict[str, Any]) -> None:
        self.config = config
        self.ollama_base = config["ollama"]["base_url"]
        self.ollama_model = config["ollama"]["model"]

        self.exit_stack = AsyncExitStack()

        # server_id -> ClientSession
        self.sessions: Dict[str, ClientSession] = {}

        # tool_name -> info {session, description, schema}
        self.tools: Dict[str, Dict[str, Any]] = {}

    async def connect_all_servers(self) -> None:
        servers_conf = self.config.get("servers", [])
        if not servers_conf:
            raise RuntimeError("config.yaml kh√¥ng c√≥ servers n√†o.")

        for srv in servers_conf:
            srv_id = srv["id"]
            cmd = srv["command"]
            args = srv.get("args", [])

            print(f"üîß Spawn MCP server [{srv_id}]: {cmd} {' '.join(args)}")

            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(
                    StdioServerParameters(
                        command=cmd,
                        args=args,
                        env=None,
                    )
                )
            )
            read, write = stdio_transport

            session = await self.exit_stack.enter_async_context(
                ClientSession(read, write)
            )
            await session.initialize()

            self.sessions[srv_id] = session

            # L·∫•y danh s√°ch tools t·ª´ server n√†y
            tools_resp = await session.list_tools()
            for t in tools_resp.tools:
                print(f"  ‚ûï Tool discovered: {t.name} ({srv_id})")
                self.tools[t.name] = {
                    "session": session,
                    "server_id": srv_id,
                    "description": t.description,
                    "schema": getattr(t, "inputSchema", None),
                }

        print(f"‚úÖ T·ªïng s·ªë tools: {len(self.tools)}")

    async def cleanup(self) -> None:
        await self.exit_stack.aclose()

    # ---------- ROUTER ----------

    def _build_tools_description_for_router(self) -> str:
        """
        Chu·∫©n b·ªã text m√¥ t·∫£ tools cho LLM router.
        """
        items = []
        for name, info in self.tools.items():
            desc = info.get("description", "")
            schema = info.get("schema")
            items.append(
                {
                    "name": name,
                    "description": desc,
                    "inputSchema": schema,
                }
            )
        return json.dumps(items, ensure_ascii=False, indent=2)

    def ask_router(self, user_query: str) -> Dict[str, Any]:
        """
        H·ªèi LLM: n√™n d√πng tool n√†o (ho·∫∑c none) + arguments g√¨.
        """
        tools_desc = self._build_tools_description_for_router()

        system_prompt = (
            "B·∫°n l√† ROUTER CHO TOOLS.\n"
            "- Nhi·ªám v·ª•: ch·ªçn ƒë√∫ng tool (ho·∫∑c 'none') v√† arguments t∆∞∆°ng ·ª©ng.\n"
            "- CH·ªà TR·∫¢ V·ªÄ JSON THU·∫¶N, KH√îNG GI·∫¢I TH√çCH, KH√îNG TEXT TH·ª™A.\n"
        )

        user_prompt = f"""
User h·ªèi: {user_query!r}

ƒê√ÇY L√Ä DANH S√ÅCH TOOLS:

{tools_desc}

Y√äU C·∫¶U:
- N·∫øu kh√¥ng tool n√†o ph√π h·ª£p, tr·∫£ v·ªÅ:
  {{"tool": "none", "arguments": {{}}}}

- N·∫øu c√≥ tool ph√π h·ª£p, tr·∫£ v·ªÅ:
  {{
    "tool": "<t√™n tool>",
    "arguments": {{
        // key:value ƒë√∫ng theo inputSchema n·∫øu c√≥
    }}
  }}

- V√≠ d·ª•: n·∫øu user h·ªèi v·ªÅ email ch∆∞a ƒë·ªçc h√¥m nay, c√≥ th·ªÉ:
  {{
    "tool": "gmail_list_today_unread",
    "arguments": {{"user_email": "synopex.no.reply@gmail.com"}}
  }}
"""
        result = call_ollama_for_json(
            self.ollama_base, self.ollama_model, user_prompt, system_prompt
        )
        # Chu·∫©n h√≥a
        tool = result.get("tool", "none")
        arguments = result.get("arguments", {})
        if not isinstance(arguments, dict):
            arguments = {}
        return {"tool": tool, "arguments": arguments}

    async def process_question(self, question: str) -> str:
        try:
            if not self.tools:
                return "‚ùå Ch∆∞a c√≥ tool n√†o ƒë∆∞·ª£c load."

            # 1. H·ªèi router
            router = self.ask_router(question)
            tool_name = router["tool"]
            arguments = router["arguments"]

            print(f"[ROUTER] tool={tool_name}, args={arguments}")

            # 2. N·∫øu router ch·ªçn "none" -> chat thu·∫ßn
            if tool_name == "none" or tool_name not in self.tools:
                answer = call_ollama_chat(
                    self.ollama_base,
                    self.ollama_model,
                    user_prompt=question,
                    system_prompt=(
                        "B·∫°n l√† tr·ª£ l√Ω AI ti·∫øng Vi·ªát th√¢n thi·ªán, ng·∫Øn g·ªçn, "
                        "gi·∫£i th√≠ch d·ªÖ hi·ªÉu."
                    ),
                )
                return answer

            # 3. G·ªçi tool t∆∞∆°ng ·ª©ng
            info = self.tools[tool_name]
            session: ClientSession = info["session"]

            try:
                tool_result = await session.call_tool(
                    tool_name, arguments=arguments
                )
            except Exception as e:
                traceback.print_exc()
                return f"‚ùå L·ªói khi g·ªçi tool '{tool_name}': {e}"

            # tool_result.structured_content ch·ª©a JSON t·ª´ server
            structured = tool_result.structuredContent

            # 4. Nh·ªù LLM format JSON th√†nh c√¢u tr·∫£ l·ªùi
            pretty = call_ollama_chat(
                self.ollama_base,
                self.ollama_model,
                user_prompt=(
                    "User h·ªèi: " + question + "\n\n"
                    "D∆∞·ªõi ƒë√¢y l√† JSON d·ªØ li·ªáu l·∫•y ƒë∆∞·ª£c t·ª´ tool "
                    f"{tool_name}:\n\n"
                    + json.dumps(structured, ensure_ascii=False, indent=2)
                    + "\n\nH√£y tr·∫£ l·ªùi user b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu. "
                      "N·∫øu l√† danh s√°ch th√¨ li·ªát k√™ r√µ r√†ng."
                ),
                system_prompt="B·∫°n l√† tr·ª£ l√Ω AI chuy√™n di·ªÖn gi·∫£i d·ªØ li·ªáu JSON cho ng∆∞·ªùi d√πng cu·ªëi.",
            )
            return pretty

        except Exception as e:
            traceback.print_exc()
            return f"‚ùå L·ªói n·ªôi b·ªô: {e}"

    async def chat_loop(self) -> None:
        print("ü§ñ Config MCP Client ƒë√£ s·∫µn s√†ng! (d√πng config.yaml)")
        print("G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t.\n")

        while True:
            try:
                user_input = input("B·∫°n: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nüëã T·∫°m bi·ªát!")
                break

            if not user_input:
                continue

            if user_input.lower() in {"exit", "quit"}:
                print("üëã T·∫°m bi·ªát!")
                break

            answer = await self.process_question(user_input)
            print(f"\nAssistant: {answer}\n")


async def main() -> None:
    cfg = load_config()
    client = ConfigMCPClient(cfg)
    try:
        await client.connect_all_servers()
        await client.chat_loop()
    finally:
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
